{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHj5aOr36PO5dZimrxt0nV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gihuni99/Pytorch_Study/blob/main/Ch6_1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AlexNet"
      ],
      "metadata": {
        "id": "I8EjXdmqZlPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AlexNet의 구조"
      ],
      "metadata": {
        "id": "Pal0wwCBZoBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://github.com/gihuni99/Pytorch_Study/assets/90080065/985ccc2c-b9ef-4ad5-8692-fbc7a32deaf7)\n",
        "\n",
        "![image](https://github.com/gihuni99/Pytorch_Study/assets/90080065/5af8c8e2-8bf8-4cfd-9946-3421dcf7aac0)\n",
        "\n",
        "**AlexNet**은 Convolutional layer에서 Activation function으로 **ReLU함수**를 사용한다. GPU-1은 color와 상관없는 정보를 추출하기 위한 커널이 학습되고, GPU-2는 주로 color와 관련된 정보를 추출하기 위한 커널이 학습된다.\n",
        "\n"
      ],
      "metadata": {
        "id": "ahf4HGWGZ2LW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 실습\n",
        "\n",
        "이전의 LeNet과 Training, Test process자체는 같다. 다마 모델의 구성이 다르기 때문에 그것에 집중에서 실습을 해보자."
      ],
      "metadata": {
        "id": "QEVUQPSwcG0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 필요 라이브러리 호출"
      ],
      "metadata": {
        "id": "3eqzC6ofcMAi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxT6n3UUX4-5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://github.com/gihuni99/Pytorch_Study/assets/90080065/1c63b97c-74ba-4a04-98d3-fd2a7f498b6a)\n",
        "\n",
        "AlexNet은 softmax함수를 통해 1000x1 vector의 output이 나오지만, 실습에서는 'Cat', 'Dog' 두가지 label만을 사용"
      ],
      "metadata": {
        "id": "JSU0-OYHcscp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Pre-processing"
      ],
      "metadata": {
        "id": "QbXX2YHHdMAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageTransform():\n",
        "    def __init__(self, resize, mean, std):\n",
        "        self.data_transform = {\n",
        "            'train': transforms.Compose([\n",
        "                transforms.RandomResizedCrop(resize, scale=(0.5, 1.0)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ]),\n",
        "            'val': transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(resize),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, img, phase):\n",
        "        return self.data_transform[phase](img)"
      ],
      "metadata": {
        "id": "-tmtxvLrdKlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AkFsn4FXeDyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_directory='/content/drive/MyDrive/Pytorch_study/data/dogs-vs-cats/Cat/'\n",
        "dog_directory='/content/drive/MyDrive/Pytorch_study/data/dogs-vs-cats/Dog/'\n",
        "\n",
        "cat_images_directory=sorted([os.path.join(cat_directory, f) for f in os.listdir(cat_directory)])\n",
        "dog_images_directory=sorted([os.path.join(dog_directory, f) for f in os.listdir(dog_directory)])\n",
        "\n",
        "images_filepaths=[*cat_images_directory,*dog_images_directory]\n",
        "correct_images_filepaths=[i for i in images_filepaths if cv2.imread(i) is not None]\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(correct_images_filepaths)\n",
        "train_images_filepath=correct_images_filepaths[:400]\n",
        "val_images_filepath=correct_images_filepaths[400:-10]\n",
        "test_images_filepath=correct_images_filepaths[-10:]\n",
        "print(len(train_images_filepath),len(val_images_filepath),len(test_images_filepath))"
      ],
      "metadata": {
        "id": "L5O141XfgX9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "실습을 진행하기 전 AlexNet은 parameter가 6000만개이다. 따라서 충분한 데이터가 없으면 성능이 좋지 않은데, 실습에서 많은 데이터를 쓸 수 없기 때문에 overfitting으로 인한 성능저하가 당연히 발생할 수 밖에 없다."
      ],
      "metadata": {
        "id": "KIYpExcIjJXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 해당 코드는 **Custom_Dataset** 클래스를 통해 image를 불러와 image preprocessing과 labeling을 진행한다. 이를 DataLoader에 전달하여 데이터를 메모리로 불러오는 것"
      ],
      "metadata": {
        "id": "R9f5V81onUO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Custom_Dataset():\n",
        "    def __init__(self,file_list,transform=None,phase='train'):\n",
        "        self.file_list=file_list #image가 존재하는 경로\n",
        "        self.transform=transform #Data preprocessing\n",
        "        self.phase=phase #'train' 또는 'val'\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path=self.file_list[idx]\n",
        "        img=Image.open(img_path) #PIL을 이용하여 image불러옴\n",
        "        img_transformed=self.transform(img,self.phase) #phase값에 따라 preprocessing과정이 달라짐(ImageTransform()에서 정의되어 있다.)\n",
        "\n",
        "        label=img_path.split('/')[-1].split('.')[0] #'/content/drive/MyDrive/Pytorch_study/data/dogs-vs-cats/Cat/cat.1.jpg'의 경로에서 label추출\n",
        "        if label=='dog':\n",
        "            label=1\n",
        "        elif label=='cat':\n",
        "            label=0\n",
        "\n",
        "        return img_transformed,label #전처리가 적용된 image와 label\n"
      ],
      "metadata": {
        "id": "XbWvuTxZjbcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing에 필요한 mean, std등의 변수 값 정의\n",
        "size=256 #model input의 크기\n",
        "mean=(0.485,0.456,0.406)\n",
        "std=(0.229,0.224,0.225)\n",
        "batch_size=32"
      ],
      "metadata": {
        "id": "JiVpklenmE-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#각 dataset별로 Custom_Dataset의 객체 생성, __getitem__()함수를 통해 값을 반환하는 것이다.\n",
        "train_dataset=Custom_Dataset(train_images_filepath,ImageTransform(resize=size,mean=mean,std=std),phase='train')\n",
        "val_dataset=Custom_Dataset(val_images_filepath,ImageTransform(resize=size,mean=mean,std=std),phase='val')\n",
        "test_dataset=Custom_Dataset(test_images_filepath,ImageTransform(resize=size,mean=mean,std=std),phase='val')\n",
        "\n",
        "print('preprocessing된 image의 크기:', train_dataset.__getitem__(0)[0].size())\n",
        "print('label:',train_dataset.__getitem__(0)[1])"
      ],
      "metadata": {
        "id": "F2bLAmALmdRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataLoader\n",
        "\n",
        "- 데이터를 메모리로 불러옴"
      ],
      "metadata": {
        "id": "AJJ71FnpF_yR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "val_dataloader=DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n",
        "test_dataloader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False)\n",
        "dataloader_dict={'train':train_dataloader,'val':val_dataloader}\n",
        "\n",
        "batch_iterator=iter(train_dataloader) #iter는 반복 가능한 객체에서 이터레이터를 반환\n",
        "inputs,label=next(batch_iterator) #iterator에서 값을 차례로 반환\n",
        "print(inputs.size())\n",
        "print(label)"
      ],
      "metadata": {
        "id": "gm-_zrLwFv_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture"
      ],
      "metadata": {
        "id": "JUc9WfkdHvhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://github.com/gihuni99/Pytorch_Study/assets/90080065/2af86c58-9f16-4889-8e7a-1c355c5b3f3b)"
      ],
      "metadata": {
        "id": "SmXPI4rEIvL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ReLU(inplace=True): inplace는 결과값을 새로운 변수에 저장하는 것이 아닌, 기존 데이터를 대체한다는 의미\n",
        "\n",
        "- self.avgpool = nn.AdaptiveAvgPool2d((6, 6)): AvgPool2d는 nn.AvgPool2d(2,stride=3) 또는 nn.AvgPool2d((2,1),stride=(3,2))로 사용하여 kernel size와 stride를 지정해준다.((2,1)은 2x1 kernel, stride=(3,2)는 H방향으로 stride=3, W방향으로 stride=2를 의미한다. 반면 nn.AdaptiveAvgPool2d는  pooling이 끝날 때의 출력을 정의한다. 즉, nn.AdaptiveAvgPool2d((6, 6))의 결과는 6x6이 된다."
      ],
      "metadata": {
        "id": "5nwovPDHI6Hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self) -> None: #->None은 함수가 반환하는 데이터 타입을 나타내는 주석과 같다. None은 반환하지 않음을 의미\n",
        "        super(AlexNet, self).__init__() #작성하지 않아도 nn.Module을 상속받는다(가독성을 높이기 위한 코드)\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256*6*6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor: #Tensor를 반환함\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "L7T0-reqHvRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Object 생성"
      ],
      "metadata": {
        "id": "2fySM7TyLgoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=AlexNet()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "am5CcCAKLgJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer & Loss Function"
      ],
      "metadata": {
        "id": "Yo7LuGABLp6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer=optim.SGD(model.parameters(),lr=0.001,momentum=0.9)\n",
        "criterion=nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "id": "87JN5gh1LtAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture 확인"
      ],
      "metadata": {
        "id": "Z9HxxubvL5lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(model,input_size=(3,256,256))"
      ],
      "metadata": {
        "id": "Z23XCP0DL_eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "JnzqDRJJMK5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,dataloader_dict,criterion,optimizer,num_epoch):\n",
        "\n",
        "    since=time.time()\n",
        "    best_acc=0.0\n",
        "\n",
        "    for epoch in range(num_epoch):\n",
        "        print('Epoch {}/{}'.format(epoch+1,num_epoch))\n",
        "        print('-'*20)\n",
        "\n",
        "        for phase in ['train','val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            epoch_loss=0.0\n",
        "            epoch_corrects=0\n",
        "\n",
        "            for inputs,labels in tqdm(dataloader_dict[phase]):\n",
        "                inputs=inputs.to(device)\n",
        "                labels=labels.to(device)\n",
        "                optimizer.zero_grad() #back propagation 전에 항상 gradient를 0으로 초기화해주어야 한다.\n",
        "\n",
        "                with torch.set_grad_enabled(phase=='train'): #autograd 활성화\n",
        "                    outputs=model(inputs)\n",
        "                    _, preds=torch.max(outputs,1) # preds=torch.max(outputs,1)로 쓰면 preds[0]은 최대값을, preds[1]은 최대값의 인덱스 즉 label을 나타낸다.\n",
        "                    loss=criterion(outputs,labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    epoch_loss += loss.item()*inputs.size(0) #inputs.size(0)은 input의 행, 즉 batch size를 의미한다. loss는 batch전체의 평균 loss이므로 batch size만큼 곱해준다.\n",
        "                    epoch_corrects+=torch.sum(preds==labels.data) #예측과 정답이 얼마나 정확한지 측정\n",
        "\n",
        "            epoch_loss=epoch_loss/len(dataloader_dict[phase].dataset) #epoch의 평균 loss를 구한다.\n",
        "            epoch_acc=epoch_corrects.double()/len(dataloader_dict[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase,epoch_loss,epoch_acc))\n",
        "    time_elapsed=time.time()-since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed//60,time_elapsed%60))\n",
        "    return model"
      ],
      "metadata": {
        "id": "U-TSxS95MKf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch=10\n",
        "model=train_model(model,dataloader_dict,criterion,optimizer,num_epoch)"
      ],
      "metadata": {
        "id": "KEN7tWRNUSN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "TTFwrKymbSVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "id_list = []\n",
        "pred_list = []\n",
        "_id = 0\n",
        "with torch.no_grad():\n",
        "    for test_path in tqdm(test_images_filepath): #test dataset사용\n",
        "        img = Image.open(test_path)\n",
        "        _id = test_path.split('/')[-1].split('.')[1]# data의 index가져오기 (Ex dog.113.jpg라는 이미지 이름에서 113)\n",
        "        transform = ImageTransform(size, mean, std)\n",
        "        img = transform(img, phase='val')# test dataset pre processing\n",
        "        img = img.unsqueeze(0)\n",
        "        img = img.to(device)\n",
        "\n",
        "        model.eval()\n",
        "        outputs = model(img)\n",
        "        preds = F.softmax(outputs, dim=1)[:, 1].tolist()\n",
        "\n",
        "        id_list.append(_id)\n",
        "        pred_list.append(preds[0])\n",
        "\n",
        "res = pd.DataFrame({\n",
        "    'id': id_list,\n",
        "    'label': pred_list\n",
        "}) # dataframe에 이미지의 id(번호)와 레이블 저장\n",
        "res.to_csv('/content/drive/MyDrive/Pytorch_study/alexnet.csv', index=False) #이미지의 id와 레이블을 alexnet.csv 파일에 저장"
      ],
      "metadata": {
        "id": "aCUz84L2bT8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataframe 확인"
      ],
      "metadata": {
        "id": "TDA26pVpb7IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res.head(10)"
      ],
      "metadata": {
        "id": "PBtAx9Ajb9No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation 결과 시각화"
      ],
      "metadata": {
        "id": "STI4xcPHcAjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_ = classes = {0:'cat', 1:'dog'}\n",
        "def display_image_grid(images_filepaths, predicted_labels=(), cols=5):\n",
        "    rows = len(images_filepaths) // cols\n",
        "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n",
        "    for i, image_filepath in enumerate(images_filepaths):\n",
        "        image = cv2.imread(image_filepath)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        a = random.choice(res['id'].values)\n",
        "        label = res.loc[res['id'] == a, 'label'].values[0]\n",
        "        if label > 0.5:\n",
        "            label = 1\n",
        "        else:\n",
        "            label = 0\n",
        "        ax.ravel()[i].imshow(image)\n",
        "        ax.ravel()[i].set_title(class_[label])\n",
        "        ax.ravel()[i].set_axis_off()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WmxI3AUZcDXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- parameter가 많지만, 실습을 위해 데이터셋 양이 적기 때문에 성능이 좋지 않다."
      ],
      "metadata": {
        "id": "kmO0Vnr2cOhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_image_grid(test_images_filepath)"
      ],
      "metadata": {
        "id": "FyaDIRbXcIT-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}